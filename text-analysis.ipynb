{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64e168d4-cbc3-4a4c-a2b2-dc46722fc824",
    "_uuid": "fbd2c55422e377772d3fc825cf473d2572b2080f"
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "270977f6-ba10-4672-b34e-abb58897d7ce",
    "_uuid": "bd2e3765fddd6f190239b2e2100e001d77851e64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import get_data_home\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "data_home = get_data_home()\n",
    "twenty_home = os.path.join(data_home, \"20news_home\")\n",
    "\n",
    "if not os.path.exists(data_home):\n",
    "    os.makedirs(data_home)\n",
    "    \n",
    "if not os.path.exists(twenty_home):\n",
    "    os.makedirs(twenty_home)\n",
    "    \n",
    "!cp ../input/20-newsgroup-sklearn/20news-bydate_py3* /tmp/scikit_learn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca1903d1-e1fa-492b-aafc-c3ffae2ce24a",
    "_uuid": "58a90ef14953d3c00d7d5c1f34bbad9547ecb62c"
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "You already learned that we have to tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b3dd90f1-a67a-49c8-9a29-94c7ec09d7e8",
    "_uuid": "15ce3cbfcae7b6af1d0f802d4c840c817b38a845"
   },
   "outputs": [],
   "source": [
    "# http://qwone.com/~jason/20Newsgroups/\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, download_if_missing=False)\n",
    "\n",
    "texts = dataset.data # Extract text\n",
    "target = dataset.target # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "871a54eb-afde-43db-8b2c-5587b213dfb1",
    "_uuid": "741517fad88d52a561884dbfdc3b8dcc82e0e886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  3 17  3  4 12  4 10 10 19]\n",
      "18846\n",
      "18846\n",
      "157\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print (target[:10])\n",
    "\n",
    "print (len(texts))\n",
    "print (len(target))\n",
    "print (len(texts[0].split()))\n",
    "print (texts[0])\n",
    "print (target[0])\n",
    "print (dataset.target_names[target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62540aeb-0b49-4171-8e7c-ac4dadce2159",
    "_uuid": "93bd1e0a8ef83c1d36d6a3a6dbf59eaa57483027"
   },
   "source": [
    "Remember we have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the 20,000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "84268d63-7f77-4bdd-97a2-a8279b340847",
    "_uuid": "fd1a639cd290c49cc97bda64efb514fafd7f299e"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6d2891a0-637d-4696-bbf5-db39250a41af",
    "_uuid": "1e64c08432e9a5404b8147a650748a3ebbaa0243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1595, 1168, 82, 20, 13]]\n",
      "18846\n",
      "160\n",
      "[14, 19415, 455, 559, 15, 29, 2552, 1240, 5609, 33, 322, 767, 2175, 2121, 871, 1343, 32, 251, 88, 77, 84, 12087, 455, 559, 15, 7, 122, 228, 63, 3, 2552, 1240, 20, 517, 3490, 50, 1, 1393, 3, 61, 437, 3, 1507, 50, 1, 1302, 2552, 3027, 3, 1, 2701, 309, 7, 122, 243, 16334, 175, 5, 4, 243, 19416, 268, 7, 122, 194, 2, 296, 37, 337, 2, 369, 4389, 22, 4, 243, 3, 7286, 12, 1, 2552, 349, 30, 20, 1502, 137, 2701, 1382, 90, 7, 397, 5987, 74, 2025, 13, 130, 56, 8, 140, 215, 90, 93, 1457, 770, 1963, 56, 8, 97, 4, 308, 9186, 1857, 2, 1306, 6, 1, 2327, 6760, 115, 348, 5987, 21, 4, 308, 3, 1857, 6, 1, 365, 658, 3, 467, 185, 1, 2552, 20, 194, 2, 1985, 1, 66, 3, 3215, 608, 7, 26, 132, 8755, 19, 2, 131, 1, 3280, 2000, 1, 1151, 1457, 770, 283, 2552, 1222]\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.texts_to_sequences(['Hello King, how are you?']))\n",
    "\n",
    "print (len(sequences))\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "76757929-31f2-4a85-929e-011499407245",
    "_uuid": "069169b21fdea43c6334ecd2c9307031c3460f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179,209 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e2509e9-01d6-4039-a790-21e0ac75e15e",
    "_uuid": "4ce7f226d4075b7e0122ba30efae47e456fd22a7"
   },
   "source": [
    "Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "adca0ca9-dcee-403c-9887-5b8102d375eb",
    "_uuid": "99510c98db9a325e2025534b3cb1ccf9d5258238",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from ratnam andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule "
     ]
    }
   ],
   "source": [
    "# Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c726fbcc-9e0d-4860-bd55-822198f38ada",
    "_uuid": "e09517f67a5014cfc9b44d4a74b2cf31048f4796"
   },
   "source": [
    "### Measuring text length\n",
    "\n",
    "Let's ensure all sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "319d4933-115a-49eb-ac34-7976e2ae2b2b",
    "_uuid": "6a2a5104a4a345c1bd93d042734b5034dba631c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292.4769712405816, 666.9329063050876)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f89e3119-5ce6-437b-8841-68e481976018",
    "_uuid": "2043270f6cd0df8392a4368fc26a3ce702a0e123"
   },
   "source": [
    "You can see, the average text is about 300 words long. However, the standard deviation is quite large which indicates that some texts are much much longer. If some user decided to write an epic novel in the newsgroup it would massively slow down training. So for speed purposes we will restrict sequence length to 100 words. You should try out some different sequence lengths and experiment with processing time and accuracy gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8318d64f-ad1b-4e75-9e56-341345c31ffb",
    "_uuid": "75e30eb5a7aab59d4dd44c3ed25e5d3a22f08e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 3]]\n",
      "[[2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(pad_sequences([[1,2,3]], maxlen=5))\n",
    "print(pad_sequences([[1,2,3,4,5,6]], maxlen=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "53020108-b7bb-4ea3-93c9-e39f59c03336",
    "_uuid": "2278bc08b537e9c80560c08a1f34a002d079302c"
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d5c77a4-0ef1-4602-9c2d-f41fb2ba2480",
    "_uuid": "1b210651bbce6277e504b0ea1eb15b7df34d820b"
   },
   "source": [
    "## Turning labels into One-Hot encodings\n",
    "\n",
    "Labels can quickly be encoded into one-hot vectors with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "9bc07094-3eea-4284-870a-1c944d890709",
    "_uuid": "4d3bb2cdd37e893a1979bdb5e605b03ca05a52f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18846, 100)\n",
      "Shape of labels: (18846, 20)\n",
      "10\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(target))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)\n",
    "\n",
    "print (target[0])\n",
    "print (labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73c95137-dd9e-4a71-a6f4-e62ff2885f15",
    "_uuid": "015296894ba4820a363af404cc0f11e7f2922949"
   },
   "source": [
    "## Loading GloVe embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "26fa63f3-bd92-4aab-9280-b270dbdee4ce",
    "_uuid": "3b981a5596e00038ed900e76be33ba89b9561cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '../input/glove-global-vectors-for-word-representation' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "e1b966ae-73a2-4f53-9cf3-682ad69a8b09",
    "_uuid": "1fdcf5986dd815d3ddfdf7e29568e4895c8b54e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.043084   0.53233    0.54254   -0.076952  -0.29673    0.52986\n",
      "  0.21379    0.15789   -0.3952    -0.91889   -0.6585     0.68706\n",
      "  0.10821   -0.10694   -0.3401     1.044      0.12775    0.51157\n",
      "  0.60314    0.71366   -0.5374     0.37737    0.12186    0.60891\n",
      "  0.50107    2.0215    -0.47318    0.46953    0.12542    0.60207\n",
      "  0.11007    0.37587    1.0137    -0.2478     0.65748    0.12801\n",
      " -0.57647   -0.25754    0.62426    0.010864  -0.40681    0.16173\n",
      " -0.84695   -0.24603    0.29078    0.8546    -0.067021   0.69331\n",
      " -0.71545   -0.25184   -0.74741   -0.26507    0.4873     0.41991\n",
      " -0.86741   -0.5235    -0.44774   -0.044584   0.033836   0.29909\n",
      "  0.73754    0.81651    0.69431    0.80453    0.29276   -0.025244\n",
      " -0.30453   -0.34329    0.11933   -0.29655    0.1072    -0.18946\n",
      "  0.18501   -0.7548    -0.25628    0.34438   -0.016743   0.0040503\n",
      "  0.39342    0.99404   -0.32159   -0.49434    0.41708   -0.011019\n",
      " -0.16613   -0.20839    0.28152   -0.82996    0.79839    0.61645\n",
      "  0.31537   -0.27629   -0.54592    0.23026    0.023473  -0.15934\n",
      " -1.4389    -0.75359    0.5149    -0.52552  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "4578a176-4e75-46e5-b827-e37a3cf12620",
    "_uuid": "e293d3a3aa91070e39d8262591eea41b9b816452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.364068\n",
      "5.197995\n",
      "4.1249743\n",
      "6.7943544\n",
      "7.3115597\n",
      "6.5261197\n",
      "7.450874\n"
     ]
    }
   ],
   "source": [
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['woman']))\n",
    "print (np.linalg.norm(embeddings_index['man'] - embeddings_index['cat']))\n",
    "\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['toad']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['man']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fog']))\n",
    "\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['fork']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['skyscraper']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "8d9a3a8f-fde2-48ce-af98-71591cdbe180",
    "_uuid": "9375f22be07d4fbba8ff316c4119e9f415ee656d"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "79244ba7299c991561eed3f57fbac27d280236f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01400006  0.078819    0.47789001 -0.71000999  0.40336999 -0.013396\n",
      " -0.070241   -0.12796    -0.80293     0.58372998  0.27814999 -1.13110006\n",
      " -1.24269998  0.065034   -0.29615    -0.21926001 -0.11177     0.20290001\n",
      " -0.30069     0.45559001  0.98092002  0.32383999  0.11154     0.42175001\n",
      " -0.71700001  1.14900005 -0.26041001  0.59004998 -0.62717998  0.089107\n",
      "  0.52561003  0.39030001 -0.10446     0.30394     0.58774     0.20553\n",
      "  0.62854999  0.40913999  0.93089998  0.68953001 -0.058053   -1.03429997\n",
      " -0.1621     -0.59283    -0.46384001 -0.12187    -0.64608997 -0.099373\n",
      "  0.32742    -0.45748001  0.11268    -0.71411997  0.54689002 -0.60856003\n",
      "  0.16841    -1.85210001  0.34494001 -0.31538001  0.72078001  0.73034\n",
      "  0.30781001 -0.40831     0.24587999 -0.396       0.82898003  0.43457001\n",
      "  1.84220004  1.47230005  0.072308   -0.074464    0.29550001  0.3768\n",
      " -0.78801     0.20651001  0.74176002 -0.61141998 -0.10625    -0.46015999\n",
      " -0.81893998 -1.03250003 -1.11150002 -1.53659999  0.20482001 -1.03869998\n",
      " -0.59808999  0.92408001  0.14132001 -0.92227    -0.062248   -0.4445\n",
      " -1.10769999  0.36662     0.35995001  1.31560004 -0.23559999  0.021989\n",
      "  0.018535   -0.21606     0.81186998 -0.88524002]\n"
     ]
    }
   ],
   "source": [
    "print (embedding_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "247e1cba-19ed-4153-8bd1-486a95e8af06",
    "_uuid": "a9fec5775edfc68a6b4f5c4c87cee9cd91248072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,172,564\n",
      "Trainable params: 172,564\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "1d2d9403-1c38-4460-b662-0a4348baafc3",
    "_uuid": "d1452fe4eea37469442d4ae4152e71098d1df900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15076 samples, validate on 3770 samples\n",
      "Epoch 1/10\n",
      "15076/15076 [==============================] - 11s 718us/step - loss: 0.1490 - categorical_accuracy: 0.2932 - val_loss: 0.1228 - val_categorical_accuracy: 0.4279\n",
      "Epoch 2/10\n",
      "15076/15076 [==============================] - 10s 634us/step - loss: 0.1021 - categorical_accuracy: 0.5493 - val_loss: 0.1007 - val_categorical_accuracy: 0.5581\n",
      "Epoch 3/10\n",
      "15076/15076 [==============================] - 9s 629us/step - loss: 0.0850 - categorical_accuracy: 0.6324 - val_loss: 0.0929 - val_categorical_accuracy: 0.5995\n",
      "Epoch 4/10\n",
      "15076/15076 [==============================] - 10s 630us/step - loss: 0.0713 - categorical_accuracy: 0.6987 - val_loss: 0.0928 - val_categorical_accuracy: 0.6188\n",
      "Epoch 5/10\n",
      "15076/15076 [==============================] - 9s 626us/step - loss: 0.0591 - categorical_accuracy: 0.7562 - val_loss: 0.0913 - val_categorical_accuracy: 0.6422\n",
      "Epoch 6/10\n",
      "15076/15076 [==============================] - 9s 624us/step - loss: 0.0472 - categorical_accuracy: 0.8074 - val_loss: 0.0958 - val_categorical_accuracy: 0.6377\n",
      "Epoch 7/10\n",
      "15076/15076 [==============================] - 9s 624us/step - loss: 0.0364 - categorical_accuracy: 0.8567 - val_loss: 0.1003 - val_categorical_accuracy: 0.6432\n",
      "Epoch 8/10\n",
      "15076/15076 [==============================] - 9s 613us/step - loss: 0.0269 - categorical_accuracy: 0.8994 - val_loss: 0.1057 - val_categorical_accuracy: 0.6496\n",
      "Epoch 9/10\n",
      "15076/15076 [==============================] - 9s 609us/step - loss: 0.0209 - categorical_accuracy: 0.9246 - val_loss: 0.1364 - val_categorical_accuracy: 0.6151\n",
      "Epoch 10/10\n",
      "15076/15076 [==============================] - 9s 610us/step - loss: 0.0182 - categorical_accuracy: 0.9347 - val_loss: 0.1281 - val_categorical_accuracy: 0.6446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4bc6083c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "\n",
    "model.fit(data, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ba271623-0278-4202-b727-51e2552f778c",
    "_uuid": "c1860d218b1d657ee634b733fdbe991ee414ec75"
   },
   "source": [
    "Our model achieves 63% accuracy on the validation set. Systems like these can be used to assign emails in customer support centers, suggest responses, or classify other forms of text like invoices which need to be assigned to an department. Let's take a look at how our model classified one of the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "f33f85c3-1abe-4d31-bba1-d401acd3202d",
    "_uuid": "3dbca1eee1d8de8a9c4da650dfbafa31913be757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: JC924@uacsc2.albany.edu\n",
      "Subject: Why are our desktop fonts changing?\n",
      "Organization: University at Albany, Albany NY 12222\n",
      "X-Newsreader: NNR/VM S_1.3.2\n",
      "Lines: 17\n",
      "\n",
      "One of our users is having an unusual problem.  If she does an Alt/Tab to\n",
      "a full-screen DOS program, when she goes back to Windows her desktop fonts\n",
      "have changed.  If she goes back to a full-screen DOS program and then goes\n",
      "back to Windows, the font has changed back to its default font.  It's not\n",
      "a major problem (everything works and the font is legible), but it is\n",
      "annoying.  Does anyone have any idea why this happens.  By the way, she\n",
      "has a DEC 486D2LP machine.\n",
      " \n",
      " \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Jeffrey M. Cohen                      Voice: 518-442-3510\n",
      "Office for Research (AD 218)          Fax:   518-442-3560\n",
      "The University at Albany              E-mail: JC924@uacsc2.albany.edu\n",
      "State University of New York\n",
      "1400 Washington Ave.\n",
      "Albany, NY 12222\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "program when she goes back to windows her desktop fonts have changed if she goes back to a full screen dos program and then goes back to windows the font has changed back to its default font it's not a major problem everything works and the font is but it is annoying does anyone have any idea why this happens by the way she has a dec machine jeffrey m cohen voice 518 442 office for research ad 218 fax 518 442 the university at albany e mail albany edu state university of new york 1400 washington ave albany ny "
     ]
    }
   ],
   "source": [
    "example = data[400] # get the tokens\n",
    "print (texts[400])\n",
    "\n",
    "# Print tokens as text\n",
    "for w in example:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "6c6f3ee8-96ef-4ece-8f1e-a08ccef0524e",
    "_uuid": "991e688ef06e9fb984786fcffee884f5c4404638"
   },
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "pred = model.predict(example.reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "1eae718f-f973-487c-a0ac-4358e16dc5af",
    "_uuid": "94726bda929e3e0c6790ad3842a6606c0e717000"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.os.ms-windows.misc'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output predicted category\n",
    "dataset.target_names[np.argmax(pred)]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
